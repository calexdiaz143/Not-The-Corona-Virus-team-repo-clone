{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_ratio(y, predictions):\n",
    "    zipped = zip(y, predictions)\n",
    "    total_points = 0\n",
    "    for curr in zipped:\n",
    "        if curr[0] == curr[1]:\n",
    "            total_points += 2\n",
    "        elif curr[0] == curr[1]+1 or curr[0] == curr[1]-1:\n",
    "            total_points += 1\n",
    "        else:\n",
    "            total_points += 0\n",
    "\n",
    "    return (total_points/(len(y)*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_remove_punctuation(input):\n",
    "    input = input.lower()\n",
    "    input = word_tokenize(input)\n",
    "    input = list(filter(lambda token: token not in string.punctuation, input))\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import word embedding packages\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import string\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "df = pd.read_csv('data/Challenge1_Training_Scenarios.csv')\n",
    "df.set_index('scenario_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab X and y values and split 80/20 train/test\n",
    "X = df['scenario']\n",
    "y = df['danger_level']\n",
    "\n",
    "train_X = X[:404]\n",
    "train_y = y[:404]\n",
    "\n",
    "test_X = X[404:]\n",
    "test_y = y[404:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bows\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1,3))\n",
    "train_counts = vect.fit_transform(train_X)\n",
    "test_counts = vect.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb.fit(train_counts, train_y)\n",
    "\n",
    "pred = clf_mnb.predict(test_counts)\n",
    "print(accuracy_ratio(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(train_counts, train_y)\n",
    "\n",
    "pred = np.rint(clf_lr.predict(test_counts))\n",
    "print(accuracy_ratio(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_kn = KNeighborsClassifier()\n",
    "clf_kn.fit(train_counts, train_y)\n",
    "\n",
    "pred = clf_kn.predict(test_counts)\n",
    "print(accuracy_ratio(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC()\n",
    "clf_svc.fit(train_counts, train_y)\n",
    "\n",
    "pred = clf_svc.predict(test_counts)\n",
    "print(accuracy_ratio(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The front desk nurse at a hospital in central Texas checks in a man who has a confirmed case of COVID-19. She keeps 6 feet of distance from him and wears a surgical mask. '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VISUALIZE DATA STUFF\n",
    "visualize = pd.DataFrame(list(zip(list(test_X), test_y, list(pred))))\n",
    "visualize['diff'] = list(np.subtract(test_y, pred))\n",
    "visualize = visualize.sort_values(by = 'diff')\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "visualize[0].loc[visualize['diff'] == 3]\n",
    "visualize[0][47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd approach\n",
    "# word2vec to embed inside/outside\n",
    "# linear regression matrix to get confidence levels of specific words\n",
    "# stemming\n",
    "# lemmatization\n",
    "# stopwords remove in/out etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab embeddings for valued words and format\n",
    "valued_negative_words = ['inside', 'crowd', 'touch', 'alcohol', 'kids', 'travel', 'airplane', 'elderly', 'illness', 'group', 'sick', 'coronavirus']\n",
    "\n",
    "for i, word in enumerate(valued_negative_words):\n",
    "    valued_negative_words[i] = glove_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to embeddings for each entry\n",
    "train_embedded_X = train_X.copy()\n",
    "for i, entry in enumerate(train_embedded_X, start=1):\n",
    "    train_embedded_X[i] = tokenize_remove_punctuation(train_embedded_X[i])\n",
    "\n",
    "    if 'covid-19' in train_embedded_X[i]:\n",
    "        train_embedded_X[i][train_embedded_X[i].index('covid-19')] = 'coronavirus'\n",
    "    if 'covid' in train_embedded_X[i]:\n",
    "        train_embedded_X[i][train_embedded_X[i].index('covid')] = 'coronavirus'\n",
    "\n",
    "    for j, word in enumerate(train_embedded_X[i]):\n",
    "        if train_embedded_X[i][j] in glove_vectors:\n",
    "            train_embedded_X[i][j] = glove_vectors[train_embedded_X[i][j]]\n",
    "        else:\n",
    "            train_embedded_X[i][j] = None\n",
    "\n",
    "            \n",
    "test_embedded_X = test_X.copy()\n",
    "for i, entry in enumerate(test_embedded_X, start=405):\n",
    "    test_embedded_X[i] = tokenize_remove_punctuation(test_embedded_X[i])\n",
    "    \n",
    "    if 'covid-19' in test_embedded_X[i]:\n",
    "        test_embedded_X[i][test_embedded_X[i].index('covid-19')] = 'coronavirus'\n",
    "    if 'covid' in test_embedded_X[i]:\n",
    "        test_embedded_X[i][test_embedded_X[i].index('covid')] = 'coronavirus'\n",
    "    \n",
    "    for j, word in enumerate(test_embedded_X[i]):\n",
    "        if test_embedded_X[i][j] in glove_vectors:\n",
    "            test_embedded_X[i][j] = glove_vectors[test_embedded_X[i][j]]\n",
    "        else:\n",
    "            test_embedded_X[i][j] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector of closest distances to each valued word approach\n",
    "for i, entry in enumerate(train_embedded_X, start=1):\n",
    "    curr_min_distance_vec = np.full(len(valued_negative_words), float('inf'))\n",
    "\n",
    "    # for each word\n",
    "    for j, word in enumerate(train_embedded_X[i]):\n",
    "        if word is not None:\n",
    "            # loop through valued words\n",
    "            for k, valued_word in enumerate(valued_negative_words):\n",
    "                curr_distance = np.sum(np.square(valued_word - word))\n",
    "                if curr_distance < curr_min_distance_vec[k]:\n",
    "                    curr_min_distance_vec[k] = curr_distance\n",
    "    train_embedded_X[i] = curr_min_distance_vec.copy()\n",
    "\n",
    "train_embedded_X = list(train_embedded_X)\n",
    "\n",
    "for i, entry in enumerate(test_embedded_X, start=405):\n",
    "    curr_min_distance_vec = np.full(len(valued_negative_words), float('inf'))\n",
    "\n",
    "    # for each word\n",
    "    for j, word in enumerate(test_embedded_X[i]):\n",
    "        if word is not None:\n",
    "            # loop through valued words\n",
    "            for k, valued_word in enumerate(valued_negative_words):\n",
    "                curr_distance = np.sum(np.square(valued_word - word))\n",
    "                if curr_distance < curr_min_distance_vec[k]:\n",
    "                    curr_min_distance_vec[k] = curr_distance\n",
    "    test_embedded_X[i] = curr_min_distance_vec.copy()\n",
    "\n",
    "test_embedded_X = list(test_embedded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24\n"
     ]
    }
   ],
   "source": [
    "embedded_clf_mnb = MultinomialNB()\n",
    "embedded_clf_mnb.fit(train_embedded_X, train_y)\n",
    "pred = embedded_clf_mnb.predict(test_embedded_X)\n",
    "print(accuracy_ratio(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cesardiaz/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# simple lr on embeddings\n",
    "embedded_clf_lr = LogisticRegression()\n",
    "embedded_clf_lr.fit(train_embedded_X, train_y)\n",
    "pred = embedded_clf_lr.predict(test_embedded_X)\n",
    "print(accuracy_ratio(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24\n"
     ]
    }
   ],
   "source": [
    "# simple naive bayes on embeddings\n",
    "embedded_clf_mnb = MultinomialNB()\n",
    "embedded_clf_mnb.fit(train_embedded_X, train_y)\n",
    "pred = embedded_clf_mnb.predict(test_embedded_X)\n",
    "print(accuracy_ratio(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cesardiaz/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# simple lr on embeddings\n",
    "embedded_clf_lr = LogisticRegression()\n",
    "embedded_clf_lr.fit(train_embedded_X, train_y)\n",
    "pred = embedded_clf_lr.predict(test_embedded_X)\n",
    "print(accuracy_ratio(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.295\n"
     ]
    }
   ],
   "source": [
    "embedded_clf_svc = SVC()\n",
    "embedded_clf_svc.fit(train_embedded_X, train_y)\n",
    "pred = embedded_clf_svc.predict(test_embedded_X)\n",
    "print(accuracy_ratio(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
