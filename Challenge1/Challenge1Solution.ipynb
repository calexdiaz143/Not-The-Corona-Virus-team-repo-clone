{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_ratio(y, predictions):\n",
    "    zipped = zip(y, predictions)\n",
    "    total_points = 0\n",
    "    for curr in zipped:\n",
    "        if curr[0] == curr[1]:\n",
    "            total_points += 2\n",
    "        elif curr[0] == curr[1]+1 or curr[0] == curr[1]-1:\n",
    "            total_points += 1\n",
    "        else:\n",
    "            total_points += 0\n",
    "\n",
    "    return (total_points/(len(y)*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_predictions(X, y, pred):\n",
    "    visualize = pd.DataFrame(list(zip(list(X), y, list(pred))))\n",
    "    visualize['diff'] = list(np.subtract(y, pred))\n",
    "    visualize = visualize.sort_values(by = 'diff')\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "    print(visualize.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import word embedding packages\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import string\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "df = pd.read_csv('data/Challenge1_Training_Scenarios.csv')\n",
    "df.set_index('scenario_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab X and y values and split 80/20 train/test\n",
    "X = df['scenario']\n",
    "y = df['danger_level']\n",
    "\n",
    "train_X = X[:404]\n",
    "train_y = y[:404]\n",
    "\n",
    "test_X = X[404:]\n",
    "test_y = y[404:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bows\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1,3))\n",
    "train_counts = vect.fit_transform(train_X)\n",
    "test_counts = vect.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "bow_mnb = MultinomialNB()\n",
    "bow_mnb.fit(train_counts, train_y)\n",
    "\n",
    "pred_bow_mnb = bow_mnb.predict(test_counts)\n",
    "print(accuracy_ratio(test_y, pred_bow_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "bow_lr = LogisticRegression()\n",
    "bow_lr.fit(train_counts, train_y)\n",
    "\n",
    "pred_bow_lr = np.rint(bow_lr.predict(test_counts))\n",
    "print(accuracy_ratio(test_y, pred_bow_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bow_kn = KNeighborsClassifier()\n",
    "bow_kn.fit(train_counts, train_y)\n",
    "\n",
    "pred_bow_kn = bow_kn.predict(test_counts)\n",
    "print(accuracy_ratio(test_y, pred_bow_kn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "bow_svc = SVC()\n",
    "bow_svc.fit(train_counts, train_y)\n",
    "\n",
    "pred_bow_svc = bow_svc.predict(test_counts)\n",
    "print(accuracy_ratio(test_y, pred_bow_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0  1  2  diff\n",
      "36  A 52 year old man goes to a farmer's market. I...  1  6    -5\n",
      "95  A 58 year old woman with blood clots goes to e...  1  6    -5\n",
      "4   A 20 year old homeless man went to the homeles...  1  6    -5\n",
      "11  A 69 year old man plays tennis with his partne...  1  6    -5\n",
      "55  An 29 year old woman with diabetes goes skiing...  1  6    -5\n"
     ]
    }
   ],
   "source": [
    "view_predictions(test_X, test_y, pred_bow_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_remove_punctuation(input):\n",
    "    input = input.lower()\n",
    "    input = word_tokenize(input)\n",
    "    input = list(filter(lambda token: token not in string.punctuation, input))\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab embeddings for valued words and format\n",
    "valued_negative_words = ['elderly', 'mask', 'social', 'coronavirus', 'home', 'work', 'outside']\n",
    "\n",
    "for i, word in enumerate(valued_negative_words):\n",
    "    valued_negative_words[i] = glove_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to embeddings for each entry\n",
    "train_embedded_X = train_X.copy()\n",
    "for i, entry in enumerate(train_embedded_X, start=1):\n",
    "    train_embedded_X[i] = tokenize_remove_punctuation(train_embedded_X[i])\n",
    "\n",
    "    if 'covid-19' in train_embedded_X[i]:\n",
    "        train_embedded_X[i][train_embedded_X[i].index('covid-19')] = 'coronavirus'\n",
    "    if 'covid' in train_embedded_X[i]:\n",
    "        train_embedded_X[i][train_embedded_X[i].index('covid')] = 'coronavirus'\n",
    "\n",
    "    for j, word in enumerate(train_embedded_X[i]):\n",
    "        if train_embedded_X[i][j] in glove_vectors:\n",
    "            train_embedded_X[i][j] = glove_vectors[train_embedded_X[i][j]]\n",
    "        else:\n",
    "            train_embedded_X[i][j] = None\n",
    "\n",
    "            \n",
    "test_embedded_X = test_X.copy()\n",
    "for i, entry in enumerate(test_embedded_X, start=405):\n",
    "    test_embedded_X[i] = tokenize_remove_punctuation(test_embedded_X[i])\n",
    "    \n",
    "    if 'covid-19' in test_embedded_X[i]:\n",
    "        test_embedded_X[i][test_embedded_X[i].index('covid-19')] = 'coronavirus'\n",
    "    if 'covid' in test_embedded_X[i]:\n",
    "        test_embedded_X[i][test_embedded_X[i].index('covid')] = 'coronavirus'\n",
    "    \n",
    "    for j, word in enumerate(test_embedded_X[i]):\n",
    "        if test_embedded_X[i][j] in glove_vectors:\n",
    "            test_embedded_X[i][j] = glove_vectors[test_embedded_X[i][j]]\n",
    "        else:\n",
    "            test_embedded_X[i][j] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector of closest distances to each valued word approach\n",
    "for i, entry in enumerate(train_embedded_X, start=1):\n",
    "    curr_min_distance_vec = np.full(len(valued_negative_words), float('inf'))\n",
    "\n",
    "    # for each word\n",
    "    for j, word in enumerate(train_embedded_X[i]):\n",
    "        if word is not None:\n",
    "            # loop through valued words\n",
    "            for k, valued_word in enumerate(valued_negative_words):\n",
    "                curr_distance = np.sum(np.square(valued_word - word))\n",
    "                if curr_distance < curr_min_distance_vec[k]:\n",
    "                    curr_min_distance_vec[k] = curr_distance\n",
    "    train_embedded_X[i] = curr_min_distance_vec.copy()\n",
    "\n",
    "train_embedded_X = list(train_embedded_X)\n",
    "\n",
    "for i, entry in enumerate(test_embedded_X, start=405):\n",
    "    curr_min_distance_vec = np.full(len(valued_negative_words), float('inf'))\n",
    "\n",
    "    # for each word\n",
    "    for j, word in enumerate(test_embedded_X[i]):\n",
    "        if word is not None:\n",
    "            # loop through valued words\n",
    "            for k, valued_word in enumerate(valued_negative_words):\n",
    "                curr_distance = np.sum(np.square(valued_word - word))\n",
    "                if curr_distance < curr_min_distance_vec[k]:\n",
    "                    curr_min_distance_vec[k] = curr_distance\n",
    "    test_embedded_X[i] = curr_min_distance_vec.copy()\n",
    "\n",
    "test_embedded_X = list(test_embedded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "embedded_mnb = MultinomialNB()\n",
    "embedded_mnb.fit(train_embedded_X, train_y)\n",
    "pred_embedded_mnb = embedded_mnb.predict(test_embedded_X)\n",
    "print(accuracy_ratio(test_y, pred_embedded_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "embedded_lr = LinearRegression()\n",
    "embedded_lr.fit(train_embedded_X, train_y)\n",
    "pred_embedded_lr = embedded_lr.predict(test_embedded_X)\n",
    "print(accuracy_ratio(test_y, pred_embedded_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.67643496, 2.7925458 , 3.42710503, 2.86616494, 3.96243855,\n",
       "       2.93841361, 4.55706271, 3.69348878, 2.41440174, 3.89641868,\n",
       "       2.79309774, 3.62734519, 3.45383744, 3.15670909, 3.66807691,\n",
       "       2.87756498, 3.330772  , 3.76304598, 2.61357825, 2.79501261,\n",
       "       3.56786778, 3.34173292, 4.15458084, 3.11201766, 3.07843819,\n",
       "       3.01378609, 3.4444302 , 1.82534976, 3.5752202 , 3.69449868,\n",
       "       3.70226748, 3.05864606, 4.2302198 , 4.72838476, 2.86869873,\n",
       "       3.40357445, 3.04338623, 3.75071324, 3.0765811 , 3.05263395,\n",
       "       3.27462303, 2.70756476, 2.90655509, 3.63332865, 4.04299814,\n",
       "       3.94373033, 3.76766954, 4.28121327, 2.76090658, 3.71365595,\n",
       "       3.74628464, 3.51316902, 3.75905777, 3.81288495, 3.44138843,\n",
       "       3.41072533, 3.23937867, 3.06730061, 3.33571318, 4.04393837,\n",
       "       3.98011626, 2.77910191, 3.90103739, 3.66311531, 3.47624922,\n",
       "       3.34366594, 3.65090252, 3.91031706, 4.45020344, 4.18591638,\n",
       "       2.60008105, 3.65993739, 3.88483474, 3.77511907, 3.99827643,\n",
       "       2.26132722, 3.54354057, 3.41616954, 3.47586784, 3.05520193,\n",
       "       3.0257651 , 2.9561163 , 4.36186665, 3.63667622, 2.88429532,\n",
       "       3.47986206, 3.74937878, 3.5641248 , 3.71644109, 3.71525441,\n",
       "       3.00849832, 4.34988989, 2.74585171, 4.58830347, 4.12867427,\n",
       "       4.22287777, 3.22743593, 3.67332525, 3.58860703, 3.66430517])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embedded_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41\n"
     ]
    }
   ],
   "source": [
    "embedded_svc = SVC()\n",
    "embedded_svc.fit(train_embedded_X, train_y)\n",
    "pred_embedded_svc = embedded_svc.predict(test_embedded_X)\n",
    "print(accuracy_ratio(test_y, pred_embedded_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
